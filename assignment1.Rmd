---
title: "Assignment1"
output: html_document
---
Load Packages
```{r}
library(RWeka)
library(knitr)
library(dplyr)
```

Load Data Sets
```{r}
sonar.df <- read.table("sonar data/sonar.all-data", sep=",")
diabetes.df <- read.table("pima-indians-diabetes data/pima-indians-diabetes.data", sep=",")
hepatitis.df <- read.table("hepatitis data/hepatitis.data", sep=",")
spect1.df <- read.table("SPECT/SPECT.train", sep=",")
spect2.df <- read.table("SPECT/SPECT.test", sep=",")
spect.df <- rbind(spect1.df, spect2.df)
```

Helper Functions:
```{r}
calculate_evaulation_metrics <- function(evaluation){
  confusionMatrix <- evaluation$confusionMatrix
  TP <- confusionMatrix[1,1]
  FN <- confusionMatrix[1,2]
  FP <- confusionMatrix[2,1]
  TN <- confusionMatrix[2,2]
  precision <- TP/(TP+FP)
  recall <- TP/(TP+FN)
  f1 <- (2*precision*recall)/(precision+recall)
  acc <- (TP+TN)/(TP+FN+FP+TN)
  data.frame(precision = precision, recall = recall, f1 = f1, accuracy = acc) 
}
```

Using Decision Trees
```{r}
decision_trees_model <- J48(V61~., data=sonar.df)
#summary(decision_trees_model)
decision_trees_evaluation <- evaluate_Weka_classifier(decision_trees_model, numFolds = 10, class = TRUE)
#summary(decision_trees_evaluation)
decision_trees_evaluation$details
decision_trees_performance <- calculate_evaulation_metrics(decision_trees_evaluation)
decision_trees_performance %>% kable
```

Using Random Forest
```{r}
random_forest_classifier <- make_Weka_classifier("weka/classifiers/trees/RandomForest")
random_forest_model <- random_forest_classifier(V61 ~ ., data = sonar.df)
#summary(random_forest_model)
randomForest_evaluation <- evaluate_Weka_classifier(random_forest_model, numFolds = 10, class = TRUE)
#summary(randomForest_evaluation)
randomForest_evaluation$details
randomForest_performance <- calculate_evaulation_metrics(randomForest_evaluation)
randomForest_performance %>% kable
```


Using Support Vector 
```{r}
SVM_model <- SMO(V61 ~ ., data = sonar.df)
#summary(random_forest_model)
SVM_evaluation <- evaluate_Weka_classifier(SVM_model, numFolds = 10, class = TRUE)
#summary(SVM_evaluation)
SVM_evaluation$details
SVM_performance <- calculate_evaulation_metrics(SVM_evaluation)
SVM_performance %>% kable
```


Using Naive Bayes
```{r}
NB_classifier <- make_Weka_classifier("weka/classifiers/bayes/NaiveBayes")
NB_model <- NB_classifier(V61 ~ ., data = sonar.df)
summary(NB_model)
NB_evaluation <- evaluate_Weka_classifier(NB_model, numFolds = 10, class = TRUE)
#summary(NB_evaluation)
NB_evaluation$details
NB_performance <- calculate_evaulation_metrics(NB_evaluation)
NB_performance %>% kable
```


Using Neural Netowrks
```{r}
NN_classifier <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
NN_model <- NN_classifier(V61 ~ ., data = sonar.df)
#summary(NN_model)
NN_evaluation <- evaluate_Weka_classifier(NN_model, numFolds = 10, class = TRUE)
#summary(NN_evaluation)
NN_evaluation$details
NN_performance <- calculate_evaulation_metrics(NN_evaluation)
NN_performance %>% kable
```

Bagging & Boosting
```{r}
#Bagging
bagging_model <- Bagging(V61 ~ ., data = sonar.df, control = Weka_control(W="J48"))
#summary(bagging_model)
bagging_evaluation <- evaluate_Weka_classifier(bagging_model, numFolds = 10, class = TRUE)
#summary(bagging_evaluation)
bagging_evaluation$details
bagging_performance <- calculate_evaulation_metrics(bagging_evaluation)
bagging_performance %>% kable

#Boosting
boosting_model <- AdaBoostM1(V61~., data=sonar.df, control = Weka_control(W = list(J48, M = 30)))
summary(boosting_model)
boosting_evaluation <- evaluate_Weka_classifier(boosting_model, numFolds = 10, class = TRUE)
#summary(boosting_evaluation)
boosting_evaluation$details
boosting_performance <- calculate_evaulation_metrics(boosting_evaluation)
boosting_performance %>% kable
```


Comprison between all for the Sonar data
```{r}
rbind(decision_trees_performance, randomForest_performance, SVM_performance, NB_performance, NN_performance, bagging_performance, boosting_performance) %>% kable
```

Results: 
Random Forest achived the best results and decision trees is the worst. But when using bagging and boosting the decison trees as the base classifer the results have been improved.


```{r}
evaluate_10time_10fold <- function(model){
  random <- 1:123
  performance.df <- data.frame(precision = numeric(0), recall = integer(0), f1 = integer(0), accuracy =  integer(0))
  for(i in 1:10){
    evaluation <- evaluate_Weka_classifier(model,seed = sample(random,1),numFolds = 10, class = TRUE)
    tmp <- calculate_evaulation_metrics(evaluation)
    performance.df <- rbind(performance.df, tmp)
  }
  performance.df
}
```

# Build Classifiers
```{r}
diabetes.df$V9 <- as.factor(diabetes.df$V9)
hepatitis.df$V1 <- as.factor(hepatitis.df$V1)
spect.df$V1 <- as.factor(spect.df$V1)

#Decision Trees
decision_trees_diabetes_model <- J48(V9~., data=diabetes.df)
decision_trees_hepatitis_model <- J48(V1~., data=hepatitis.df)
decision_trees_spect_model <- J48(V1~., data=spect.df)
decision_trees_sonar_model <- J48(V61~., data=sonar.df)

decision_trees_sonar_performance <- evaluate_10time_10fold(decision_trees_sonar_model)
decision_trees_diabetes_performance <- evaluate_10time_10fold(decision_trees_diabetes_model)
decision_trees_hepatities_performance <- evaluate_10time_10fold(decision_trees_hepatitis_model)
decision_trees_spect_performance <- evaluate_10time_10fold(decision_trees_spect_model)

#Random Forest
random_forest_diabetes_model <- random_forest_classifier(V9~., data=diabetes.df)
random_forest_hepatitis_model <- random_forest_classifier(V1~., data=hepatitis.df)
random_forest_spect_model <- random_forest_classifier(V1~., data=spect.df)
random_forest_sonar_model <- random_forest_classifier(V61~., data=sonar.df)

random_forest_sonar_performance <- evaluate_10time_10fold(random_forest_sonar_model)
random_forest_diabetes_performance <- evaluate_10time_10fold(random_forest_diabetes_model)
random_forest_hepatities_performance <- evaluate_10time_10fold(random_forest_hepatitis_model)
random_forest_spect_performance <- evaluate_10time_10fold(random_forest_spect_model)

#Support Vector Machines
SMO_diabetes_model <- SMO(V9~., data=diabetes.df)
SMO_hepatitis_model <- SMO(V1~., data=hepatitis.df)
SMO_spect_model <- SMO(V1~., data=spect.df)
SMO_sonar_model <- SMO(V61~., data=sonar.df)

SMO_sonar_performance <- evaluate_10time_10fold(SMO_sonar_model)
SMO_diabetes_performance <- evaluate_10time_10fold(SMO_diabetes_model)
SMO_hepatities_performance <- evaluate_10time_10fold(SMO_hepatitis_model)
SMO_spect_performance <- evaluate_10time_10fold(SMO_spect_model)

#Naive Bias
NB_diabetes_model <- NB_classifier(V9~., data=diabetes.df)
NB_hepatitis_model <- NB_classifier(V1~., data=hepatitis.df)
NB_spect_model <- NB_classifier(V1~., data=spect.df)
NB_sonar_model <- NB_classifier(V61~., data=sonar.df)

NB_sonar_performance <- evaluate_10time_10fold(NB_sonar_model)
NB_diabetes_performance <- evaluate_10time_10fold(NB_diabetes_model)
NB_hepatities_performance <- evaluate_10time_10fold(NB_hepatitis_model)
NB_spect_performance <- evaluate_10time_10fold(NB_spect_model)

#Neural Networks
# note: Takes alot of time to compute
#NN_diabetes_model <- NN_classifier(V9~., data=diabetes.df)
#NN_hepatitis_model <- NN_classifier(V1~., data=hepatitis.df)
#NN_spect_model <- NN_classifier(V1~., data=spect.df)
#NN_sonar_model <- NN_classifier(V61~., data=sonar.df)

#NB_sonar_performance <- NN_diabetes_model(NB_sonar_model)
#NB_diabetes_performance <- NN_diabetes_model(NB_diabetes_model)
#NB_hepatities_performance <- NN_hepatitis_model(NB_hepatitis_model)
#NB_spect_performance <- NN_spect_model(NB_spect_model)

#Bagging
bagging_diabetes_model <- Bagging(V9~., data=diabetes.df, control = Weka_control(W = list(J48, M = 30)))
bagging_hepatitis_model <- Bagging(V1~., data=hepatitis.df, control = Weka_control(W = list(J48, M = 30)))
bagging_spect_model <- Bagging(V1~., data=spect.df, control = Weka_control(W = list(J48, M = 30)))
bagging_sonar_model <- Bagging(V61~., data=sonar.df, control = Weka_control(W = list(J48, M = 30)))

bagging_sonar_performance <- evaluate_10time_10fold(bagging_sonar_model)
bagging_diabetes_performance <- evaluate_10time_10fold(bagging_diabetes_model)
bagging_hepatities_performance <- evaluate_10time_10fold(bagging_hepatitis_model)
bagging_spect_performance <- evaluate_10time_10fold(bagging_spect_model)

#Boosting
boosting_diabetes_model <- AdaBoostM1(V9~., data=diabetes.df, control = Weka_control(W = list(J48, M = 30)))
boosting_hepatitis_model <- AdaBoostM1(V1~., data=hepatitis.df, control = Weka_control(W = list(J48, M = 30)))
boosting_spect_model <- AdaBoostM1(V1~., data=spect.df, control = Weka_control(W = list(J48, M = 30)))
boosting_sonar_model <- AdaBoostM1(V61~., data=sonar.df, control = Weka_control(W = list(J48, M = 30)))

boosting_sonar_performance <- evaluate_10time_10fold(boosting_sonar_model)
boosting_diabetes_performance <- evaluate_10time_10fold(boosting_diabetes_model)
boosting_hepatities_performance <- evaluate_10time_10fold(boosting_hepatitis_model)
boosting_spect_performance <- evaluate_10time_10fold(boosting_spect_model)
```

#Accuarcy Matrix
```{r}
accuracy.df <- data.frame(datasetName = character(0), decision_tree  = integer(0), random_forest = integer(0), svm =  integer(0), naive_bayes = integer(0), bagging = integer(0), boosting = integer(0))
#Sonar
accuracy_sonar <- data.frame(datasetName ="Sonar",decision_tree = mean(decision_trees_sonar_performance$accuracy),random_forest = mean(random_forest_sonar_performance$accuracy), svm = mean(SMO_sonar_performance$accuracy), naive_bayes = mean(NB_sonar_performance$accuracy), bagging = mean(bagging_sonar_performance$accuracy), boosting =mean(boosting_sonar_performance$accuracy))
accuracy.df <-  rbind(accuracy.df, accuracy_sonar)
#Diabetes
accuracy_diabetes <- data.frame(datasetName ="Diabetes",decision_tree = mean(decision_trees_diabetes_performance$accuracy),random_forest = mean(random_forest_diabetes_performance$accuracy), svm = mean(SMO_diabetes_performance$accuracy), naive_bayes = mean(NB_diabetes_performance$accuracy), bagging = mean(bagging_diabetes_performance$accuracy), boosting =mean(boosting_diabetes_performance$accuracy))
accuracy.df <-  rbind(accuracy.df, accuracy_diabetes)
#Hepatitis
accuracy_hepatities <- data.frame(datasetName= "Hepatitis",decision_tree =mean(decision_trees_hepatities_performance$accuracy),random_forest = mean(random_forest_hepatities_performance$accuracy), svm = mean(SMO_hepatities_performance$accuracy), naive_bayes = mean(NB_hepatities_performance$accuracy),  bagging = mean(bagging_hepatities_performance$accuracy),boosting = mean(boosting_hepatities_performance$accuracy))
accuracy.df <-  rbind(accuracy.df, accuracy_hepatities)
#SPECT
accuracy_spect <- data.frame(datasetName = "SPECT",decision_tree= mean(decision_trees_spect_performance$accuracy),random_forest = mean(random_forest_spect_performance$accuracy),svm =  mean(SMO_spect_performance$accuracy),naive_bayes = mean(NB_spect_performance$accuracy),bagging =  mean(bagging_spect_performance$accuracy), boosting = mean(boosting_spect_performance$accuracy))
accuracy.df <-  rbind(accuracy.df, accuracy_spect)
accuracy.df %>% kable
``` 

#Precision Matrix
```{r}
precision.df <- data.frame(datasetName = character(0), decision_tree  = integer(0), random_forest = integer(0), svm =  integer(0), naive_bayes = integer(0), bagging = integer(0), boosting = integer(0))
#Sonar
precision_sonar <- data.frame(datasetName ="sonar",decision_tree = mean(decision_trees_sonar_performance$precision),random_forest = mean(random_forest_sonar_performance$precision), svm = mean(SMO_sonar_performance$precision), naive_bayes = mean(NB_sonar_performance$precision), bagging = mean(bagging_sonar_performance$precision), boosting =mean(boosting_sonar_performance$precision))
precision.df <-  rbind(precision.df, precision_sonar)
#Diabetes
precision_diabetes <- data.frame(datasetName ="Diabetes",decision_tree = mean(decision_trees_diabetes_performance$precision),random_forest = mean(random_forest_diabetes_performance$precision), svm = mean(SMO_diabetes_performance$precision), naive_bayes = mean(NB_diabetes_performance$precision), bagging = mean(bagging_diabetes_performance$precision), boosting =mean(boosting_diabetes_performance$precision))
precision.df <-  rbind(precision.df, precision_diabetes)
#Hepatitis
precision_hepatities <- data.frame(datasetName= "Hepatitis",decision_tree =mean(decision_trees_hepatities_performance$precision),random_forest = mean(random_forest_hepatities_performance$precision), svm = mean(SMO_hepatities_performance$precision), naive_bayes = mean(NB_hepatities_performance$precision),  bagging = mean(bagging_hepatities_performance$precision),boosting = mean(boosting_hepatities_performance$precision))
precision.df <-  rbind(precision.df, precision_hepatities)
#SPECT
precision_spect <- data.frame(datasetName = "SPECT",decision_tree= mean(decision_trees_spect_performance$precision),random_forest = mean(random_forest_spect_performance$precision),svm =  mean(SMO_spect_performance$precision),naive_bayes = mean(NB_spect_performance$precision),bagging =  mean(bagging_spect_performance$precision), boosting = mean(boosting_spect_performance$precision))
precision.df <-  rbind(precision.df, precision_spect)
precision.df %>% kable
```

#Recall Matrix
```{r}
recall.df <- data.frame(datasetName = character(0), decision_tree  = integer(0), random_forest = integer(0), svm =  integer(0), naive_bayes = integer(0), bagging = integer(0), boosting = integer(0))

#Sonar
recall_sonar <- data.frame(datasetName ="sonar",decision_tree = mean(decision_trees_sonar_performance$recall),random_forest = mean(random_forest_sonar_performance$recall), svm = mean(SMO_sonar_performance$recall), naive_bayes = mean(NB_sonar_performance$recall), bagging = mean(bagging_sonar_performance$recall), boosting =mean(boosting_sonar_performance$recall))
recall.df <-  rbind(recall.df, recall_sonar)
#Diabetes
recall_diabetes <- data.frame(datasetName ="Diabetes",decision_tree = mean(decision_trees_diabetes_performance$recall),random_forest = mean(random_forest_diabetes_performance$recall), svm = mean(SMO_diabetes_performance$recall), naive_bayes = mean(NB_diabetes_performance$recall), bagging = mean(bagging_diabetes_performance$recall), boosting =mean(boosting_diabetes_performance$recall))
recall.df <-  rbind(recall.df, recall_diabetes)
#Hepatitis
recall_hepatities <- data.frame(datasetName= "Hepatitis",decision_tree =mean(decision_trees_hepatities_performance$recall),random_forest = mean(random_forest_hepatities_performance$recall), svm = mean(SMO_hepatities_performance$recall), naive_bayes = mean(NB_hepatities_performance$recall),  bagging = mean(bagging_hepatities_performance$recall),boosting = mean(boosting_hepatities_performance$recall))
recall.df <-  rbind(recall.df, recall_hepatities)
#SPECT
recall_spect <- data.frame(datasetName = "SPECT",decision_tree= mean(decision_trees_spect_performance$recall),random_forest = mean(random_forest_spect_performance$recall),svm =  mean(SMO_spect_performance$recall),naive_bayes = mean(NB_spect_performance$recall),bagging =  mean(bagging_spect_performance$recall), boosting = mean(boosting_spect_performance$recall))
recall.df <-  rbind(recall.df, recall_spect)
recall.df %>% kable
```

#F1 Matrix
```{r}
f1.df <- data.frame(datasetName = character(0), decision_tree  = integer(0), random_forest = integer(0), svm =  integer(0), naive_bayes = integer(0), bagging = integer(0), boosting = integer(0))
#Sonar
f1_sonar <- data.frame(datasetName ="sonar",decision_tree = mean(decision_trees_sonar_performance$f1),random_forest = mean(random_forest_sonar_performance$f1), svm = mean(SMO_sonar_performance$f1), naive_bayes = mean(NB_sonar_performance$f1), bagging = mean(bagging_sonar_performance$f1), boosting =mean(boosting_sonar_performance$f1))
f1.df <-  rbind(f1.df, f1_sonar)
#Diabetes
f1_diabetes <- data.frame(datasetName ="Diabetes",decision_tree = mean(decision_trees_diabetes_performance$f1),random_forest = mean(random_forest_diabetes_performance$f1), svm = mean(SMO_diabetes_performance$f1), naive_bayes = mean(NB_diabetes_performance$f1), bagging = mean(bagging_diabetes_performance$f1), boosting =mean(boosting_diabetes_performance$f1))
f1.df <-  rbind(f1.df, f1_diabetes)
#Hepatitis
f1_hepatities <- data.frame(datasetName= "Hepatitis",decision_tree =mean(decision_trees_hepatities_performance$f1),random_forest = mean(random_forest_hepatities_performance$f1), svm = mean(SMO_hepatities_performance$f1), naive_bayes = mean(NB_hepatities_performance$f1),  bagging = mean(bagging_hepatities_performance$f1),boosting = mean(boosting_hepatities_performance$f1))
f1.df <-  rbind(f1.df, f1_hepatities)
#SPECT
f1_spect <- data.frame(datasetName = "SPECT",decision_tree= mean(decision_trees_spect_performance$f1),random_forest = mean(random_forest_spect_performance$f1),svm =  mean(SMO_spect_performance$f1),naive_bayes = mean(NB_spect_performance$f1),bagging =  mean(bagging_spect_performance$f1), boosting = mean(boosting_spect_performance$f1))
f1.df <-  rbind(f1.df, f1_spect)
f1.df %>% kable
```

Results:
The Random Forest SVM and boosting achieved good results specially in the F-score measure.
The Decision trees achived the worst results

###T-Test
We will check if two algorithms are statistically significant by applyting t-test and checking if the p-value less thatn 0.05 

Sonar DataSet
```{r}
decision_trees_sonar <- c(decision_trees_sonar_performance$accuracy, decision_trees_sonar_performance$recall, decision_trees_sonar_performance$precision, decision_trees_sonar_performance$f1)

random_forest_sonar <- c(random_forest_sonar_performance$accuracy, random_forest_sonar_performance$recall, random_forest_sonar_performance$precision, random_forest_sonar_performance$f1)

NB_sonar <- c(NB_sonar_performance$accuracy, NB_sonar_performance$recall, NB_sonar_performance$precision, NB_sonar_performance$f1)

bagging_sonar <- c(bagging_sonar_performance$accuracy, bagging_sonar_performance$recall, bagging_sonar_performance$precision, bagging_sonar_performance$f1)

boosting_sonar <- c(boosting_sonar_performance$accuracy, boosting_sonar_performance$recall, boosting_sonar_performance$precision, boosting_sonar_performance$f1)

SMO_sonar <- c(SMO_sonar_performance$accuracy, SMO_sonar_performance$recall, SMO_sonar_performance$precision, SMO_sonar_performance$f1)
sprintf("P-Value: Decision Tree vs Random Forest %0.8f",t.test(decision_trees_sonar,random_forest_sonar)$p.value )
sprintf("P-Value: Decision Tree vs NB %0.8f",t.test(decision_trees_sonar,NB_sonar)$p.value)
sprintf("P-Value: Decision Tree vs SMO %0.8f",t.test(decision_trees_sonar,SMO_sonar)$p.value)
sprintf("P-Value: Decision Tree vs Bagging %0.8f",t.test(decision_trees_sonar,bagging_sonar)$p.value)
sprintf("P-Value: Decision Tree vs Boosting %0.8f",t.test(decision_trees_sonar,boosting_sonar)$p.value)
sprintf("P-Value: Random Forest vs NB %0.8f",t.test(random_forest_sonar,NB_sonar)$p.value)
sprintf("P-Value: Random Forest vs SMO %0.8f",t.test(random_forest_sonar,SMO_sonar)$p.value)
sprintf("P-Value: Random Forest vs Bagging %0.8f",t.test(random_forest_sonar,bagging_sonar)$p.value)
sprintf("P-Value: Random Forest vs Boosting %0.8f",t.test(random_forest_sonar,boosting_sonar)$p.value)
sprintf("P-Value: NB vs SMO %0.8f",t.test(NB_sonar,SMO_sonar)$p.value)
sprintf("P-Value: NB vs Bagging %0.8f",t.test(NB_sonar,bagging_sonar)$p.value)
sprintf("P-Value: NB vs Boosting %0.8f",t.test(NB_sonar,boosting_sonar)$p.value)
sprintf("P-Value: NB vs Bagging %0.8f",t.test(SMO_sonar,bagging_sonar)$p.value)
sprintf("P-Value: NB vs Boosting %0.8f",t.test(NB_sonar,boosting_sonar)$p.value)
```


###Is there a clear and only winner?
There are no clear winner as the perfomrance of the algorithms are quite close to each other.
For example:
Random forest achieved better results in the sonar dataset. However SVM and Boosting achived better results in the rest of the data sets.
So, every data set has its own algorithm that which achives better results.

###Interpret the results.
Overall Random Forest and SVM were good solutions specially in the F-score measure.
The Neural netowrk achived good results but duo it complexity couldn't be tested duo 10 times 10-fold.

